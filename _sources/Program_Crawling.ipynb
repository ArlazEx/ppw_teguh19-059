{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e8c2eb",
   "metadata": {},
   "source": [
    "# Program Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a5a6b",
   "metadata": {},
   "source": [
    "<p>Langkat pertama yang dilakukan yaitu crawling untuk mendapatkan link yang nantinya untuk dicrawling teks yang diinginkan.<p>\n",
    "<p>Untuk menjalankan program dibawah ialah dengan memberikan perintah lewat CMD yang telah masuk ke direktori file tersimpan. Dengan mengetik \"scrapyrun spider namafilepynya.py -o namafilebaru.csv\"</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881eb445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        start_url = [\n",
    "                'https://pta.trunojoyo.ac.id/welcome/index/31',\n",
    "                'https://pta.trunojoyo.ac.id/welcome/index/32',\n",
    "                'https://pta.trunojoyo.ac.id/welcome/index/33',\n",
    "                'https://pta.trunojoyo.ac.id/welcome/index/34',\n",
    "                'https://pta.trunojoyo.ac.id/welcome/index/35',\n",
    "                ]\n",
    "        for url in start_url:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for item in response.css('#content_journal > ul > li'):\n",
    "            yield {\n",
    "                'Link': item.css(f'div:nth-child(3) > div:nth-child(5) > a::attr(href)').get(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf799e81",
   "metadata": {},
   "source": [
    "<p>Setelah mendapatkan file csv dari program pertama maka akan dilakukan crawling ke dua untuk mendpatkan data yang diinginkan</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f9c94",
   "metadata": {},
   "source": [
    "<p>Untuk hasil_link.csv namanya dapat diganti sesuai dari penamaan dokumen crawling yang pertama. Untuk crawlingnya sendiri caranya sama dengan yang pertama</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quote\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        csvdata = pd.read_csv('hasil_link.csv').values\n",
    "        start_url = [ link[0] for link in csvdata ]\n",
    "\n",
    "        for url in start_url:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        yield {\n",
    "            'Judul': response.css('#content_journal > ul > li > div:nth-child(2) > a::text').get(),\n",
    "            'Abstrak': response.css('#content_journal > ul > li > div:nth-child(4) > div:nth-child(2) > p::text').get()\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
